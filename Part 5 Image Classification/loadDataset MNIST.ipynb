{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdffce14-3f7d-45e4-b273-4ad1b38e0bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor,transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import random\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0f4d0f8-012e-4de0-9570-242ff6d24673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model without softmax because nn.CrossEntropyLoss() have included\n",
    "class ImageClassificationModel2 (nn.Module):\n",
    "    def __init__(self,input_shape,output_shape):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(start_dim = 1, end_dim = -1),  # x_first_batch.shape = torch.Size([32, 3, 32, 32]) should pick(3,32,32). Thus, change start_dim to 1\n",
    "            nn.Linear(in_features = input_shape, out_features = output_shape)\n",
    "            # ,nn.Softmax(dim = 1) # model(x_first_batch).shape = torch.Size([32, 10]), should pick 10. Thus, change to dim = 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faf656c8-6373-4cbf-bbc8-eb533b3e53d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(87)\n",
    "model2 = ImageClassificationModel2(784,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87286fa7-bb72-49c2-86da-d145b2dc6d40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load train result\n",
    "state_dict = torch.load(\"image_classification_weights.pth\", weights_only=True)\n",
    "model2.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab7cccf8-35c2-4139-a66b-b3486c89ce4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer_stack.1.weight',\n",
       "              tensor([[-0.0353,  0.0279, -0.0032,  ..., -0.0251,  0.0327, -0.0172],\n",
       "                      [-0.0107, -0.0177,  0.0131,  ...,  0.0322, -0.0352,  0.0179],\n",
       "                      [-0.0196,  0.0092,  0.0163,  ..., -0.0203,  0.0294, -0.0308],\n",
       "                      ...,\n",
       "                      [ 0.0022,  0.0249, -0.0205,  ...,  0.0311,  0.0052, -0.0214],\n",
       "                      [ 0.0281,  0.0322,  0.0321,  ...,  0.0145, -0.0040,  0.0085],\n",
       "                      [ 0.0242,  0.0077, -0.0030,  ...,  0.0064,  0.0204,  0.0201]])),\n",
       "             ('layer_stack.1.bias',\n",
       "              tensor([-0.0864,  0.2267, -0.0180, -0.1125,  0.0773,  0.2201, -0.0384,  0.1551,\n",
       "                      -0.3890, -0.0361]))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eacee58e-9462-47e5-b4a7-cce1f1125977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3397e4b-f582-4e39-8bf2-e2c8cb0b96b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the same transform used during training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # For grayscale MNIST-like data\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load and preprocess the image\n",
    "img_path = \"archive/MNIST_dataset/test/5/11.png\" #this is image '6'\n",
    "image = Image.open(img_path)\n",
    "input_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "#img = transform(image)  shape: [1, 28, 28]  (C, H, W)\n",
    "#input_tensor = img.unsqueeze(0) shape: [1, 1, 28, 28] (B, C, H, W)\n",
    "\n",
    "\n",
    "#Expected input of size [batch, channels, height, width]\n",
    "#without batch, it will prompt error, Add a fake “batch” dimension to make it valid. [batch, channels, height, width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7aa61617-3730-4d8b-bb16-1a7d7e5ed621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([5.9399], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([5]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model2(input_tensor)\n",
    "torch.max(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5c1859c-a5af-4757-9f1b-b444f765f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model2(input_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    pred_class = predicted.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ee5aae7-6f1e-43ff-91cf-654adccc0dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.title(f\"Predicted class: {pred_class}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5debbb-1af4-43f4-a606-9fed102e3813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
